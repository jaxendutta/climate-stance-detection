{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultra-Lightweight Cross-lingual Stance Detection Model Development\n",
    "\n",
    "This notebook implements an extremely memory-efficient approach to developing a cross-lingual stance detection model using a smaller XLM-RoBERTa model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Enable mixed precision training\n",
    "scaler = torch.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5645, Validation: 807, Test: 1613\n",
      "Stance labels: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed data\n",
    "train_data = pd.read_csv('../data/processed/train.csv')\n",
    "val_data = pd.read_csv('../data/processed/val.csv')\n",
    "test_data = pd.read_csv('../data/processed/test.csv')\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# Encode stance labels\n",
    "le = LabelEncoder()\n",
    "train_data['stance_encoded'] = le.fit_transform(train_data['stance'])\n",
    "val_data['stance_encoded'] = le.transform(val_data['stance'])\n",
    "test_data['stance_encoded'] = le.transform(test_data['stance'])\n",
    "\n",
    "print(f\"Stance labels: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u1/a9dutta/miniconda3/envs/my_jupyter_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tokenized and prepared for model training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the tokenizer (using a smaller model)\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "def tokenize_data(texts, labels, max_length=128):  # Reduced max_length\n",
    "    if isinstance(texts, pd.Series):\n",
    "        texts = texts.astype(str).tolist()\n",
    "    else:\n",
    "        texts = [str(text) for text in texts]\n",
    "    \n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    return TensorDataset(encodings['input_ids'], encodings['attention_mask'], torch.tensor(labels))\n",
    "\n",
    "# Tokenize the data\n",
    "train_dataset = tokenize_data(train_data['processed_text'], train_data['stance_encoded'])\n",
    "val_dataset = tokenize_data(val_data['processed_text'], val_data['stance_encoded'])\n",
    "test_dataset = tokenize_data(test_data['processed_text'], test_data['stance_encoded'])\n",
    "\n",
    "print(\"Data tokenized and prepared for model training.\")\n",
    "\n",
    "# Clear some memory\n",
    "del train_data, val_data, test_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Model and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Use a smaller model\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=len(le.classes_))\n",
    "model.to(device)\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=5, lr=1e-5, accumulation_steps=8):  # Increased accumulation steps\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_loader) * epochs // accumulation_steps\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            with torch.cuda.amp.autocast():  # Mixed precision\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss / accumulation_steps\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), '../models/xlm_roberta_stance_detection_best.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    return classification_report(true_labels, predictions, target_names=le.classes_, digits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u1/a9dutta/miniconda3/envs/my_jupyter_env/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/5:   0%|          | 0/1412 [00:00<?, ?it/s]/tmp/ipykernel_138334/588711968.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Mixed precision\n",
      "/u1/a9dutta/miniconda3/envs/my_jupyter_env/lib/python3.12/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/u1/a9dutta/miniconda3/envs/my_jupyter_env/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Epoch 1/5:  11%|█▏        | 160/1412 [01:42<14:03,  1.48it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # Reduced batch size\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "trained_model = train(model, train_loader, val_loader, epochs=5)  # Reduced epochs\n",
    "\n",
    "print(\"Model training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('../models/xlm_roberta_stance_detection_best.pth'))\n",
    "\n",
    "evaluation_report = evaluate(model, test_loader)\n",
    "print(\"Model Evaluation Report:\")\n",
    "print(evaluation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results and Next Steps\n",
    "\n",
    "Based on the evaluation results, analyze the model's performance:\n",
    "\n",
    "1. Overall accuracy: [Your observation]\n",
    "2. Performance across different stances: [Your observation]\n",
    "3. Performance across different languages: [Your observation]\n",
    "\n",
    "Next steps:\n",
    "1. If the model performance is satisfactory, proceed to create a stance detection script\n",
    "2. If not, consider further optimizations or using a different approach (e.g., distilled models, or non-transformer based methods)\n",
    "3. Analyze misclassifications to understand model weaknesses\n",
    "4. Consider data augmentation or additional preprocessing steps if needed\n",
    "\n",
    "With this model developed and evaluated, you can now move on to creating a script for detecting stances in new, unseen data, assuming the performance is acceptable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_jupyter_env)",
   "language": "python",
   "name": "my_jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
