{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Multilingual Stance Detection\n",
    "\n",
    "This notebook handles the preprocessing of our collected Reddit data, including:\n",
    "1. Text cleaning\n",
    "2. Stance determination\n",
    "3. Feature extraction\n",
    "4. `train`/`val`/`test` splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import joblib\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 8079 samples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>language</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>collected_at</th>\n",
       "      <th>title_length</th>\n",
       "      <th>body_length</th>\n",
       "      <th>language_verified</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>total_length</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wu9zny</td>\n",
       "      <td>The r/climatechange Verified User Flair Program</td>\n",
       "      <td>r/climatechange is a community centered around...</td>\n",
       "      <td>41</td>\n",
       "      <td>20</td>\n",
       "      <td>en</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>2024-11-07 18:05:59.306484</td>\n",
       "      <td>47</td>\n",
       "      <td>3164</td>\n",
       "      <td>True</td>\n",
       "      <td>verified</td>\n",
       "      <td>3212</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1gm271u</td>\n",
       "      <td>1.5C is dead. The climate fight isn’t.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79</td>\n",
       "      <td>18</td>\n",
       "      <td>en</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>2024-11-07 18:06:00.729802</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>too_short</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1glbafl</td>\n",
       "      <td>I’m incredibly sad for our environment today.</td>\n",
       "      <td>Trump has all but signed a death warrant for o...</td>\n",
       "      <td>1521</td>\n",
       "      <td>359</td>\n",
       "      <td>en</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>2024-11-07 18:06:00.729809</td>\n",
       "      <td>46</td>\n",
       "      <td>527</td>\n",
       "      <td>True</td>\n",
       "      <td>verified</td>\n",
       "      <td>573</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1glc2te</td>\n",
       "      <td>I’m sad. We really do live in a post-truth wor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>787</td>\n",
       "      <td>102</td>\n",
       "      <td>en</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>2024-11-07 18:06:00.731326</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>verified</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1glsfp1</td>\n",
       "      <td>Anthropogenic warming has ushered in an era of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>climatechange</td>\n",
       "      <td>2024-11-07 18:06:00.732643</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>verified</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0   wu9zny    The r/climatechange Verified User Flair Program   \n",
       "1  1gm271u             1.5C is dead. The climate fight isn’t.   \n",
       "2  1glbafl     I’m incredibly sad for our environment today.    \n",
       "3  1glc2te  I’m sad. We really do live in a post-truth wor...   \n",
       "4  1glsfp1  Anthropogenic warming has ushered in an era of...   \n",
       "\n",
       "                                                body  score  num_comments  \\\n",
       "0  r/climatechange is a community centered around...     41            20   \n",
       "1                                                NaN     79            18   \n",
       "2  Trump has all but signed a death warrant for o...   1521           359   \n",
       "3                                                NaN    787           102   \n",
       "4                                                NaN     34             1   \n",
       "\n",
       "  language      subreddit                collected_at  title_length  \\\n",
       "0       en  climatechange  2024-11-07 18:05:59.306484            47   \n",
       "1       en  climatechange  2024-11-07 18:06:00.729802            38   \n",
       "2       en  climatechange  2024-11-07 18:06:00.729809            46   \n",
       "3       en  climatechange  2024-11-07 18:06:00.731326           109   \n",
       "4       en  climatechange  2024-11-07 18:06:00.732643            95   \n",
       "\n",
       "   body_length  language_verified verification_status  total_length  stance  \n",
       "0         3164               True            verified          3212       0  \n",
       "1            0               True           too_short            38       0  \n",
       "2          527               True            verified           573       1  \n",
       "3            0               True            verified           108       1  \n",
       "4            0               True            verified            95       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "latest_file = max(glob.glob('../data/processed/preprocessed_stance/*.csv'), key=os.path.getctime)\n",
    "data = pd.read_csv(latest_file)\n",
    "print(f\"Loaded {len(data)} samples\")\n",
    "\n",
    "# Display first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stance distribution:\n",
      "stance\n",
      " 0    6787\n",
      " 1    1081\n",
      "-1     211\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Stance distribution by language:\n",
      "stance     -1     0    1\n",
      "language                \n",
      "de         38  1829  126\n",
      "en        148  3146  677\n",
      "es          0   117   13\n",
      "fr         20   879   88\n",
      "it          5   816  177\n"
     ]
    }
   ],
   "source": [
    "# Display stance distribution\n",
    "print(\"\\nStance distribution:\")\n",
    "print(data['stance'].value_counts())\n",
    "print(\"\\nStance distribution by language:\")\n",
    "print(pd.crosstab(data['language'], data['stance']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n",
      "Processing en texts...\n",
      "en: 15000 features\n",
      "Processing es texts...\n",
      "es: 4455 features\n",
      "Processing de texts...\n",
      "de: 15000 features\n",
      "Processing fr texts...\n",
      "fr: 15000 features\n",
      "Processing it texts...\n",
      "it: 15000 features\n"
     ]
    }
   ],
   "source": [
    "# Prepare text\n",
    "texts = data['title'] + ' ' + data['body'].fillna('')\n",
    "languages = data['language']\n",
    "\n",
    "# Create separate vectorizers for each language\n",
    "print(\"Creating TF-IDF features...\")\n",
    "X_by_language = {}\n",
    "\n",
    "for lang in set(languages):\n",
    "    print(f\"Processing {lang} texts...\")\n",
    "    # Get texts for this language\n",
    "    lang_mask = languages == lang\n",
    "    lang_texts = texts[lang_mask]\n",
    "    \n",
    "    # Create and fit vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=15000,\n",
    "        ngram_range=(1, 3),\n",
    "        lowercase=True,\n",
    "        strip_accents='unicode'\n",
    "    )\n",
    "    \n",
    "    # Transform texts\n",
    "    X_lang = vectorizer.fit_transform(lang_texts)\n",
    "    X_by_language[lang] = {\n",
    "        'vectorizer': vectorizer,\n",
    "        'features': X_lang,\n",
    "        'indices': np.where(lang_mask)[0]\n",
    "    }\n",
    "    \n",
    "    print(f\"{lang}: {X_lang.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining features...\n",
      "Combined feature matrix shape: (8079, 15000)\n"
     ]
    }
   ],
   "source": [
    "# Combine all features into one sparse matrix\n",
    "print(\"Combining features...\")\n",
    "n_samples = len(texts)\n",
    "n_features = max(X_lang['features'].shape[1] for X_lang in X_by_language.values())\n",
    "X = sp.lil_matrix((n_samples, n_features))\n",
    "\n",
    "for lang_data in X_by_language.values():\n",
    "    features = lang_data['features']\n",
    "    indices = lang_data['indices']\n",
    "    X[indices, :features.shape[1]] = features\n",
    "\n",
    "# Convert to CSR format for efficiency\n",
    "X = X.tocsr()\n",
    "print(f\"Combined feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n",
      "Training samples: 5655\n",
      "Validation samples: 1212\n",
      "Test samples: 1212\n"
     ]
    }
   ],
   "source": [
    "# Get labels\n",
    "y = data['stance']\n",
    "\n",
    "# Split the data\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data...\n",
      "\n",
      "Preprocessing complete!\n",
      "Feature dimensionality: 15000\n"
     ]
    }
   ],
   "source": [
    "# Save the processed data\n",
    "print(\"Saving processed data...\")\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "processed_data = {\n",
    "    'train': (X_train, y_train),\n",
    "    'val': (X_val, y_val),\n",
    "    'test': (X_test, y_test),\n",
    "    'vectorizers': {lang: data['vectorizer'] for lang, data in X_by_language.items()},\n",
    "    'feature_size': n_features\n",
    "}\n",
    "\n",
    "joblib.dump(processed_data, '../data/processed/processed_data.joblib')\n",
    "\n",
    "print(\"\\nPreprocessing complete!\")\n",
    "print(f\"Feature dimensionality: {n_features}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_jupyter_env)",
   "language": "python",
   "name": "my_jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
